# U-Net: A PyTorch Implementation in 60 lines of Code

1. TOC 
{:toc}

## Introduction
Today's blog post is going to be short and sweet. Today, we will be looking at how to implement the [U-Net architecture](https://arxiv.org/abs/1505.04597) in PyTorch in 60 lines of code. 

This blog is not an introduction to Image Segmentation or theoritical explaination of the U-Net architecture, for that, I would like to refer the reader to [this wonderful article](https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47) by Harshall Lamba. Rather, this blog post is a step-by-step explaination of how to implement U-Net from scratch in PyTorch.

In this blogpost - first, we will understand the U-Net architecture - specifically, the input and output shapes of each block. Next, we will translate this understanding to concise PyTorch code. 

I also share a working notebook to train this implementation of U-Net of [SIIM ACR Pneumothorax](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation) Kaggle competition [here](). 

So, let's get started.

## Understanding Input and Output shapes in U-Net

![](/images/unet.png "fig-1 The U-Net Architecture")

As can be seen from `fig-1`, the architecture is "U-shaped", hence the name "U-Net". The complete architecture consists of two parts - the Encoder and the Decoder. The Encoder is like any standard CNN - such as ResNet, that extracts a meaningful feature map from an input image. As is standard practice for a CNN, the Encoder, doubles the number of channels at every step and halves the spatial dimension. 

Next, the Decoder actually upsamples the feature maps, where at every step, it doubles the spatial dimension and halves the number of channels (opposite to that of what an Encoder does). 

![](/images/Unet-self.png "fig-2 The U-Net Architecture: Simplified")

`fig-2` is meant to be the same as `fig-1` but in `fig-2` I have tried to simplify the U-Net architecture a little. The `Black dots` represent the `Encoder` or `Decoder` Blocks. 

We can think of this whole architecture as a factory line where the Black dots represents stations and the path itself is a conveyor belt where different actions take place to the Image depending on whether the conveyor belt is Yellow or Orange.

The Yellow highlight represents a MaxPool operation. Therefore, in the `Encoder` path, the Image itself loses becomes half the size of what it used to be. 

The Orange highlight on the other hand represents the `UpConv` operation, where the Image becomes twice the size it used to be and loses half the number of channels. 

In the Decoder side (all Orange path), at every station (Black dot), the outputs from the `Encoder` path are also concatenated to the input of the Decoder station/block. This is also very clear from `fig-1` where the blue rectangle represents the input to the `DecoderBlock` and the white is the output coming from the `EncoderBlock`.

Also, at every block/station - there are two 2D Convolution operations, without padding. The first `conv` operation changes the number of channels in the input Image from `in_channel` to `out_channel` while the second convolution operation keeps the same number of channels from `out_channel` to `out_channel`.

Let's start to now turn this understanding to PyTorch Code. 

## The Black Dots / Block

As mentioned, all black dots have two convolution operation. First that changes the number of channels from `in_channel` to `out_channel` and second that keeps the number of channels the same as `out_channel`. Also, there is `ReLU` activation between the two Convolution layers.

Note that the `Block` (black dots in `fig-2`) does the same task irrespective of whether it is the Encoder or the Decoder path. The only difference is that in the `Decoder` path, the input feature map coming to the `Block` is concatenated with output from the `Encoder` Block.

Let's write `Block` in code. 

```python
class Block(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)
        self.relu  = nn.ReLU()
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)
    
    def forward(self, x):
        return self.conv2(self.relu(self.conv1(x)))
```

That's simple - two convolution operations, one that changes number of channels from `in_ch` to `out_ch` and another that goes from `out_ch` to `out_ch`. Both are 2D convolutions with kernel size 3 and no padding as mentioned in the paper with a `ReLU` activation between them.

Let's make sure this works. 

```python
enc_block = Block(1, 64)
x         = torch.randn(1, 1, 572, 572)
enc_block(x).shape

>> torch.Size([1, 64, 568, 568])
```

So this is looking good, the output size matches that in `fig-1` top-left. Given an input image with size `1x572x572` the output is of size `64x568x568`. Remember, all we have to do is pass `in_ch` and `out_ch` parameters, and the `Block` itself performs two `Conv2D` operation with `ReLU` activation between them. 

## The Encoder
Now that we have implemented the `Block`s or the black dots in `fig-2`, we are ready to implement the `Encoder`. All we have to do is add the `yellow` highlight or `MaxPool` operation from `fig-2` and we should be good. 

```python
class Encoder(nn.Module):
    def __init__(self, chs=(3,64,128,256,512,1024)):
        super().__init__()
        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])
        self.pool       = nn.MaxPool2d(2)
    
    def forward(self, x):
        ftrs = []
        for block in self.enc_blocks:
            x = block(x)
            ftrs.append(x)
            x = self.pool(x)
        return ftrs
```

And that's exactly what we have done. The `self.enc_blocks` is a list of `Block`s that goes from `in_ch` to `out_ch` depending on `chs`. The `chs` variable represents the Channels. As can be seen from `fig-1`, the channels go from 3 - 64 - 128 - 256 - 512 - 1024, therefore, `chs` is a tuple of these numbers `(3,64,128,256,512,1024)` and we define the `Block`s accordingly. For example, the first block takes in 3 channel image and outputs 64 channel feature map, the second block takes in 64 channels feature map and outputs 128 channels feature map and so on. 

Next, we also add the `self.pool` or the Yellow highlight operation in `fig-2` which represents the `MaxPool` operation that is applied to the output of every `Block` before it goes in to the next `Block`. 

Therefore, the `forward` operation, performs the `Block` opreation, saves the feature map output and applies the `self.pool` operation. 

Great, we have just implemented the `Encoder` in `U-Net`. 

Let's make sure it works. 

```python
encoder = Encoder()
x    = torch.randn(1, 3, 572, 572)
ftrs = encoder(x)
for ftr in ftrs: print(ftr.shape)

>> 
torch.Size([1, 64, 568, 568])
torch.Size([1, 128, 280, 280])
torch.Size([1, 256, 136, 136])
torch.Size([1, 512, 64, 64])
torch.Size([1, 1024, 28, 28])
```

The output matches the dimensions mentioned in `fig-1`, so we are on the right track! 


## THe Decoder

Note that the Decoder `Block` or the black dot in the `Decoder` does the same job as that in the `Encoder`, the only difference being the grey arrows in `fig-1` or `fig-2`. We have to make sure that the input of every Decoder `Block` is concatenated with the output from the corresponding `Encoder` block before the operation is performed. 

So, let's implement the `Decoder`.

```python
class Decoder(nn.Module):
    def __init__(self, chs=(1024, 512, 256, 128, 64)):
        super().__init__()
        self.chs         = chs
        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])
        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) 
        
    def forward(self, x, encoder_features):
        for i in range(len(self.chs)-1):
            x        = self.upconvs[i](x)
            enc_ftrs = self.crop(encoder_features[i], x)
            x        = torch.cat([x, enc_ftrs], dim=1)
            x        = self.dec_blocks[i](x)
        return x
    
    def crop(self, enc_ftrs, x):
        _, _, H, W = x.shape
        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)
        return enc_ftrs
```

Since the number of channels in Decoder go from 1024 - 512 - 256 - 128 - 64, therefore the `chs` variable in `Decoder` is a tuple reflecting exactly that. The `self.dec_blocks` therefore, is a list of Decoder `Block`s that go from `in_ch` to `out_ch`. For example, the first block as shown in `fig-1` accepts 1024 channel feature map outputs 512 channel feature map, the second block accepts 512 feature map and outputs 256 channel feature map and so on.

These `self.dec_blocks` represents all the Black dots on the decoder side in `fig-2` so far. We still have to add all the Orange path operations in `fig-2` that is, we still need to the Up Sampling. And we also need to do the concatenation (grey arrows) of the input of decoder block with the output of encoder block.

Note that, at every Decoder `Block`, if the input size is 1024 channels, then the actual `Decoder` output is only 512 channels and the rest of the 512 channels are coming from the `Encoder`. Therefore, the UpConv operation halves the number of channels and also doubles the spatial dimensions of the Image. Note that, since the number of channels at every orange path are going to be different in the `Decoder`, we have the `self.upconvs` list, that is nothing but an UpSampling or `nn.ConvTranspose2d` operation of kernel size 2 and stride 2 as mentioned in the research paper that goes from `in_ch` to `out_ch` as well. 

So first, if the input to the Decoder Block is 1024 channels large, it is first changed to be of 512 channels large feature map. Note that the outputs of Encoder Block from `fig-1` are cropped before they are concatented with this output from the up-sampling operation. Therefore, we do the `self.crop` operation to make sure that the encoder features `enc_ftrs` and the output of the up-sampling operation `X` have the same spatial dimension. Finally, they are concatenated and the result is passed to the `Block` to perform two convolutions and single ReLU activation on it. That's really all that goes into implementing the `Decoder`.


Let's make sure this works so far. Note that for the `Decoder` we also need to input the `encoder_features` that were output from the `Encoder`. 

```python
decoder = Decoder()
x = torch.randn(1, 1024, 28, 28)
decoder(x, ftrs[::-1][1:]).shape

>> (torch.Size([1, 64, 388, 388])
```

And there it is, the final feature map is of size `64x388x388` which matches that of `fig-1`. We have just successfully implemented both the `Encoder` and the `Decoder` so far.

## U-Net 
Now that we have everything we need to build our `U-Net`, let's just quickly do it. 

```python
class UNet(nn.Module):
    def __init__(self, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1, 
                 retain_dim=False):
        super().__init__()
        self.encoder     = Encoder(enc_chs)
        self.decoder     = Decoder(dec_chs)
        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)
        self.retain_dim  = retain_dim

    def forward(self, x):
        enc_ftrs = self.encoder(x)
        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])
        out      = self.head(out)
        if self.retain_dim:
            out = F.interpolate(out, self.out_sz)
        return out
```

That's really all it takes to implement the U-Net Architecture. The `Encoder` accepts the `enc_chs` and `Decoder` accepts the `dec_chs`. Finally since at the first Decoder block, the recieves the input from the second last Encoder block, the output of the second Decoder block receives the inputs from the third last Encoder block and so on, therefore the `enc_ftrs` are reversed before they are fed to the Decoder. Also, since the first `Decoder` block accepts the last outputs and the second-last `Encoder` block outputs, they are fed to the `decoder` and that's really it. 

We have just successfully implemented the `U-Net` architecture in PyTorch. Everything put together, this looks something like: 

```python
class Block(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)
        self.relu  = nn.ReLU()
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)
    
    def forward(self, x):
        return self.conv2(self.relu(self.conv1(x)))


class Encoder(nn.Module):
    def __init__(self, chs=(3,64,128,256,512,1024)):
        super().__init__()
        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])
        self.pool       = nn.MaxPool2d(2)
    
    def forward(self, x):
        ftrs = []
        for block in self.enc_blocks:
            x = block(x)
            ftrs.append(x)
            x = self.pool(x)
        return ftrs


class Decoder(nn.Module):
    def __init__(self, chs=(1024, 512, 256, 128, 64)):
        super().__init__()
        self.chs         = chs
        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])
        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) 
        
    def forward(self, x, encoder_features):
        for i in range(len(self.chs)-1):
            x        = self.upconvs[i](x)
            enc_ftrs = self.crop(encoder_features[i], x)
            x        = torch.cat([x, enc_ftrs], dim=1)
            x        = self.dec_blocks[i](x)
        return x
    
    def crop(self, enc_ftrs, x):
        _, _, H, W = x.shape
        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)
        return enc_ftrs


class UNet(nn.Module):
    def __init__(self, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1, 
                 retain_dim=False):
        super().__init__()
        self.encoder     = Encoder(enc_chs)
        self.decoder     = Decoder(dec_chs)
        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)
        self.retain_dim  = retain_dim

    def forward(self, x):
        enc_ftrs = self.encoder(x)
        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])
        out      = self.head(out)
        if self.retain_dim:
            out = F.interpolate(out, self.out_sz)
        return out
```

This turns out to be a single screen of code and can be trained using the **SIIM ACR Pneumothorax** training script that I shared in my last blog.

## Conclusion
I hope that today, I was able to provide a concise and easy to digest implementation of the U-Net architecture with proper explainations of every line of code. 

For a complete working notebook to train this implementation, refer [here](https://github.com/amaarora/amaarora.github.io/blob/master/nbs/Training.ipynb). 

As usual, in case I have missed anything or to provide feedback, please feel free to reach out to me at [@amaarora](https://twitter.com/amaarora).

Also, feel free to [subscribe to my blog here](https://amaarora.github.io/subscribe) to receive regular updates regarding new blog posts. Thanks for reading!