# Squeeze and Excitation Networks Explained with PyTorch Implementation

1. TOC 
{:toc}

## Introduction
In this blog post, we will be looking at SeNet architecture from the research paper [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507) and re-implement it from scratch in [PyTorch](https://pytorch.org/). 

From the paper:
> Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ∼25%.

Remember [ResNet](https://arxiv.org/abs/1512.03385)? SeNet builds on top of the popular ResNet architecture to also add weights to each channel of a convolution block, so that the network can also perform feature recalibration - or also take into account the relationship between channels.

## Squeeze-and-Excitation Block

The main idea behind the paper can be explained using the image below:

![](/images/senet_block.png "fig1: Squeeze-and-Excitation Block")

1. Take an input image `X` and perform a convolution operation to generate `U`. 
2. Perform the **squeeze** operation - reduce the image from `C x H x W` to `C x 1 x 1`  
3. Perform the **excitation** operation - take the output of **squeeze** operation and generate per-channel weights
4. Apply the weights to the feature map `U` to generate the SE Block output.

In other words, what an SE Block does is that it adds a set of weights to each channel of the input. So unlike a CNN where, each channel has equal weights, the output of SE Block is weighted based on the channel weights. 

The authors suggest that such a weighting for different channels performs different operations at dufferent depths inside the network. From the paper:
> In earlier layers, it excites infor- mative features in a class-agnostic manner, strengthening the shared low-level representations. In later layers, the SE blocks become increasingly specialised, and respond to dif- ferent inputs in a highly class-specific manner. As a consequence, the benefits of the feature recalibration performed by SE blocks can be accumulated through the network.

Also, from the paper:
> We expect the learning of convolutional features to be enhanced by explicitly modelling channel interdependencies, so that the network is able to increase its sensitivity to informative features which can be exploited by subsequent transformations.

Let's quickly look at the **Squeeze** and **Excitation** blocks in a little more detail.

### Squeeze: Global Information Embedding
When performing convolution operation, since each filter operates on a local receptive field, it is unable to look at the information outside this region.

To mitigate this problem, the authors proposed to **squeeze** the global spatial information into a channel descriptor. This is achieved by using global average pooling to generate channel-wise statistics. 

### Excitation: Adaptive Recalibration
To make use of the global information, the authors followed the **squeeze** operation with **excitation** operation which aims to generate weights per channel and capture channel-wise dependencies. From the paper:

> To meet these criteria, we opt to employ a simple gating mechanism with a sigmoid activation:

![](/images/gating_operation.png "fig3: Excitation/gating operation")

where:
`δ` refers to ReLU operation
`σ` refers to Sigmoid operation 
`W1` and `W2` are two fully-connected layers

The two FC layers form a bottleneck-architecture, that is, the first `W1` layer is used for dimensionality reduction by a ratio `r` and the second `W2` layer is a dimensionality-increasing layer returning to the channel dimension of `U`.

Since, the Sigmoid layer would return numbers between 0-1, these are the channel weights and the final output of the block is obtained by:

![](/images/senet_output.png "fig4: SeNet output")

That is a weighted sum based on the channel weights.

Let's implement this in PyTorch to bolster our understanding.

## Squeeze-and-Excitation Block in PyTorch

```python
class SELayer(nn.Module):
    "From https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py"
    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False), # W1 dimensionality-reduction layer 
            nn.ReLU(inplace=True), # δ (ReLu non linearity)
            nn.Linear(channel // reduction, channel, bias=False), # W2 dimensionality-increasing layer
            nn.Sigmoid() # σ (Sigmoid non linearity to get channel weights)
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        ## squeeze operation to reduce `bc x c x h x w` to `bs x c`
        y = self.avg_pool(x).view(b, c)      
        ## excitation operation to get weights per channel and view as `bs x c x 1 x 1`
        y = self.fc(y).view(b, c, 1, 1)
        ## final channel weighted output
        return x * y.expand_as(x)
```

Allow me please to explain this implementation below:

### Squeeze Excitation Explained

![](/images/squeeze_excite.jpg "fig5: Squeeze Excitation operation with output shapes")

Let's consider we have an input image of shape 3x224x224 and the batch size is 1. Let's pass this input image through to a convolution filter with output channels 64. So, we get the output 1x64x224x224. Excellent, now on to the Squeeze and Excitation operation. 

We perform the **Squeeze** operation as mentioned to reduce the image to `bs x c` that is, 1x64. This is done by applying the Average Pooling operation to the input image. The 1x64 matrix contains global information of the image as take the mean accross `HxW` for all 64 channels. 

Now, we perform the **Excitation** operation to get the channel weights. Remember fig-3?

![](/images/gating_operation.png "fig3: Excitation/gating operation")

```python
self.fc = nn.Sequential(
    nn.Linear(channel, channel // reduction, bias=False),  
    nn.ReLU(inplace=True), 
    nn.Linear(channel // reduction, channel, bias=False), 
    nn.Sigmoid()
)
```
The `self.fc` performs exactly this operation. First, we reduce the dimension by `reduction=16` in this case. So we reduce the `1x64` to `1x4` matrix. Next, we apply the ReLU operation to get `1x4` matrix as this operation does not update shape but is a non linear operation. Next, we pass the `1x4` through to the dimensionality-increasing layer to get `1x64` matrix back. This is the bottleneck operation to first reduce the matrix from `1x64` to `1x4` and then increase the dimensions back to `1x64`. Finally, we apply the `Sigmoid` operation to get the channel weights. We broadcast this `1x64` matrix to `1x64x224x224` shape and return the final output of the SE Module by doing element-wise multiplication with input `X`.   

That's all there is to the Squeeze and Excitation networks. 

The authors mention that this Squeeze-and-Excitation operation or the SE block can be integrated into standard architectures such as VGGNet, InceptionNet or even ResNet. 

Instead of the `conv1` example operation in our case, we simply replace it with the **Inception** or **Residual** block like so:

![](/images/se-resnet.png "fig6: Se-Inception and Se-ResNet Modules")


