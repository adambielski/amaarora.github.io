# Squeeze and Excitation Networks Explained +  PyTorch Implementation

1. TOC 
{:toc}

## Introduction
In this blog post, we will be looking at SeNet architecture from the research paper [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507) and re-implement it from scratch in [PyTorch](https://pytorch.org/). 

From the paper:
> Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of âˆ¼25%.

Remember [ResNet](https://arxiv.org/abs/1512.03385)? SeNet builds on top of the popular ResNet architecture to also add weights to each channel of a convolution block, so that the network can also perform feature recalibration - or also take into account the relationship between channels.

## Squeeze-and-Excitation Block

The main idea behind the paper can be explained using the image below:

![](/images/senet_block.png "fig1: Squeeze-and-Excitation Block")

1. Take an input image `X` and perform a convolution operation to generate `U`. 
2. Perform the **squeeze** operation - reduce the image from `C x H x W` to `C x 1 x 1`  
3. Perform the **excitation** operation - take the output of **squeeze** operation and generate per-channel weights
4. Apply the weights to the feature map `U` to generate the SE Block output.

In other words, what an SE Block does is that it adds a set of weights to each channel of the input. So unlike a CNN where, each channel has equal weights, the output of SE Block is weighted based on the channel weights. 

The authors suggest that such a weighting for different channels performs different operations at dufferent depths inside the network. From the paper:
> In earlier layers, it excites infor- mative features in a class-agnostic manner, strengthening the shared low-level representations. In later layers, the SE blocks become increasingly specialised, and respond to dif- ferent inputs in a highly class-specific manner. As a consequence, the benefits of the feature recalibration performed by SE blocks can be accumulated through the network.