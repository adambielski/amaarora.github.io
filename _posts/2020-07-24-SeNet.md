# Squeeze and Excitation Networks Explained with PyTorch Implementation

1. TOC 
{:toc}

## Introduction 
In this blog post, we will be looking at the [Squeeze-and-Excitation](https://arxiv.org/abs/1709.01507) networks. We will refer to the [research paper by Hu et al](https://arxiv.org/abs/1709.01507) and understand what **Squeeze-and-Excitation** networks are theoritically before implementing the novel architecture in PyTorch with very few modifications to the popular `ResNet` architecture.

## Main idea
So, what's new in the Squeeze-and-Excitation networks? How are they different from the ResNet architecture? 

Let's consider an RGB image as an input. Then the convolution operation with a 3x3 kernel on the input image can be visualised as below:

![](/images/cnn.gif "fig-1: Convolution operation on RGB image; src: https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1")

A feature map is generated per-channel (RGB) and then summed together to form one channel or the final output of the convolution operation. 

![](/images/output.gif "fig-2: Convolution output; src: https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1")

The important point to note, is that in a standard Convolution operation, each of the channels get equal weights. 

**Main Idea behind Se-Nets**:
Here's the main idea behind SE-Nets: 
> It is possible that one of the channels might have more relevant information than the others. So, instead of giving each channel equal weights, it might be beneficial if more attention is paid to some channels and less attention is paid to the others. 

![](/images/senet_block.png "fig-3: Squeeze-and-Excitation block")

This main idea can be further explained using the **Squeeze-and-Excitation block** image above from the paper. First, a feature transformation (such as a convolution operation) is performed on the input image `X` to get features `U`. Next, we perform a **squeeze** operation to get a single value for each channel. After, we perform an **excitation** operation on the output of the **squeeze** operation to get per-channel weights. Finally, the output of the **Squeeze-and-Excitation block** is the weighted sum per-channel. In the image above, notice how each channel has a distinct colour where this colour corresponds to the channel weights. 

From the paper: 
> The role this operation performs at different depths differs throughout the network. In earlier layers, it excites informative features in a class-agnostic manner, strengthening the shared low-level representations. In later layers, the SE blocks become increasingly specialised, and respond to different inputs in a highly class-specific manner. As a consequence, the benefits of the feature recalibration performed by SE blocks can be accumulated through the network.