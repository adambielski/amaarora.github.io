# Squeeze and Excitation Networks Explained with PyTorch Implementation

1. TOC 
{:toc}

## Introduction 
In this blog post, we will be looking at the [Squeeze-and-Excitation](https://arxiv.org/abs/1709.01507) networks. We will refer to the [research paper by Hu et al](https://arxiv.org/abs/1709.01507) and first understand what **Squeeze-and-Excitation** networks are  before implementing the novel architecture in PyTorch with very few modifications to the popular `ResNet` architecture.

First, we develop an intuition for what SE-Nets are and the novel idea behind their success. Next, we will look at the **Squeeze** and **Excitation** operations in a little more detail. Finally, we implement the **Squeeze-and-Excitation** networks in PyTorch with very minor updates to the **Basic Block** for [ResNet](https://arxiv.org/abs/1512.03385).

## Intuition behind Squeeze-and-Excitation Networks
So, what's new in the Squeeze-and-Excitation networks? How are they different from the ResNet architecture? 

Let's consider an RGB image as an input. Then the convolution operation with a 3x3 kernel on the input image can be visualised as below:

![](/images/cnn.gif "fig-1: Convolution operation on RGB image; src: https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1")

A feature map is generated per-channel (RGB) and then summed together to form one channel or the final output of the convolution operation. 

![](/images/output.gif "fig-2: Convolution output; src: https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1")

The important point to note is that in a standard Convolution operation **each of the channels get equal weights**. 

**Main Idea behind Se-Nets**:

> It is possible that one of the channels might have more relevant information than the others. So, instead of giving each channel equal weights, it might be beneficial if more attention is paid to some channels and less attention is paid to the others. 

![](/images/senet_block.png "fig-3: Squeeze-and-Excitation block")

This main idea can be further explained using the **Squeeze-and-Excitation block** image above from the paper. First, a feature transformation (such as a convolution operation) is performed on the input image `X` to get features `U`. Next, we perform a **squeeze** operation to get a single value for each channel. After, we perform an **excitation** operation on the output of the **squeeze** operation to get per-channel weights. Finally, the output of the **Squeeze-and-Excitation block** is the weighted sum per-channel. In the image above, notice how each channel has a distinct colour where this colour corresponds to the channel weights. 

From the paper: 
> The role this operation performs at different depths differs throughout the network. In earlier layers, it excites informative features in a class-agnostic manner, strengthening the shared low-level representations. In later layers, the SE blocks become increasingly specialised, and respond to different inputs in a highly class-specific manner. As a consequence, the benefits of the feature recalibration performed by SE blocks can be accumulated through the network.

Now, we will look at the **Squeeze** and **Excitation** operations in a little more detail. 

## Squeeze: Global Information Embedding

The main purpose of the Squeeze operation is to extract global information from each of the channels of an image. Since, convolution is a local operation (that is, at a particular time, it is only able to see a part of the image), it might be beneficial to also extract information outside the receptive field of the convolution filter. 

The **Squeeze** operation is meant to do exactly that and the authors keep it as simple as possible. 

The authors perform a Global Average Pooling operation to reduce the `C x H x W` image to `C x 1 x 1` to get a global statistic for each channel. 

Formally, the Global Average Pooling or the Squeeze operation can be represented as: 

![](/images/squeeze.png "eq-1: Global Average Pooling")

In other words, all we are doing is taking the mean of each channel across `H x W` spatial dimensions. 

## Excitation: Adaptive Recalibration 

Now that we have a vector of length `C` from the [Squeeze]() operation, the next step is to generate a set of weights for each channel. This is done with the help of **Excitation** operation explained in this section. 

Formally, the excitation operation can be represented by:

![](/images/gating_operation.png "eq-2: Excitation")

where:	
- `δ` refers to ReLU operation	
- `σ` refers to Sigmoid operation 	
- `W1` and `W2` are two fully-connected layers	
- `z` is the output from the Squeeze block

The two FC layers form a bottleneck architecture, that is, the first `W1` layer is used for dimensionality reduction by a ratio `r` and the second `W2` layer is a dimensionality increasing layer returning to the channel dimension of `U`.	

Since, the Sigmoid layer would return numbers between 0 and 1, these are the channel weights and the final output of the block can be respresented as:	

![](/images/senet_output.png "fig4: SeNet output")

From the paper: 
> The excitation operator maps the input-specific descriptor z to a set of channel weights. In this regard, SE blocks intrinsically introduce dynamics conditioned on the input, which can be regarded as a self-attention function on channels whose relationships are not confined to the local receptive field the convolutional filter are responsive to.

## Squeeze and Excitation Block in PyTorch

```python
class SE_Block(nn.Module):
    "credits: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py#L4"
    def __init__(self, c, r=16):
        super().__init__()
        self.squeeze = nn.AdaptiveAvgPool2d(1)
        self.excitation = nn.Sequential(
            nn.Linear(c, c // r, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(c // r, c, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        bs, c, _, _ = x.shape
        y = self.squeeze(x).view(bs, c)
        y = self.excitation(y).view(bs, c, 1, 1)
        return x * y.expand_as(x)
```

As mentioned the **Squeeze** operation is a global Average Pooling operation and in PyTorch this can be represented as `nn.AdaptiveAvgPool2d(1)` where 1, represents the output size. 

Next, the **Excitation** network is a bottle neck architecture with two FC layers, first to reduce the dimensions and second to increase the dimensions back to original. We reduce the dimensions by a reduction ratio `r=16`. This is as simple as creating a `nn.Sequential` with two FC layers, with a `nn.ReLU()` in between and followed by a `nn.Sigmoid()`.

The outputs of the **Excitation** operation are the channel weights which are then multiplied element-wise to input feature `X` to get the final output of the `SE_Block`.


## SE Block with Existing SOTA Architectures

From the paper:
> The structure of the SE block is simple and can be used directly in existing state-of-the-art architectures by replacing components with their SE counterparts, where the performance can be effectively enhanced. SE blocks are also computationally lightweight and impose only a slight increase in model complexity and computational burden.

In other words, it is really simple to integrate SE blocks with existing state-of-art architectures. The authors provide two examples for SE-ResNet and SE-Inception as below:

![](/images/se-resnet.png "fig5: SE-Inception and SE-ResNet Architectures")

In this blogpost we will implement the SE-ResNet architecture and implementing the SE-Inception architecture is left as an experiment to the reader.  

The authors experimented by inserting SE block in various positions and found that the performance improvements produced by SE units are fairly robust to their location, provided that they are applied prior to branch aggregation. In this post, we will construct a SE-ResNet architecture using the **Standard SE block** integration as below: 

![](/images/SE_block_integ.png "fig-6: SE block integrations ablation study")

**Key point to note:**
> As can be seen from the Standard SE block integration, the SE block is followed directly by the Residual operation in ResNet before summation with the identity branch. 

## SE-ResNet in PyTorch

As we saw from fig-6, we simply insert the SE block after the Residual operation. To create SE-Resnet34 and below, we simply copy the `BasicBlock` from torchvision from [here](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L35).

```python
class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,
                 base_width=64, dilation=1, norm_layer=None):
        super(BasicBlock, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError('BasicBlock only supports groups=1 and base_width=64')
        if dilation > 1:
            raise NotImplementedError("Dilation > 1 not supported in BasicBlock")
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = norm_layer(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out
```

Next, we update the `forward` to insert the SE block operation as in fig-6. 

```python 
class SEBasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,
                 base_width=64, dilation=1, norm_layer=None, r=16):
        super(SEBasicBlock, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError('BasicBlock only supports groups=1 and base_width=64')
        if dilation > 1:
            raise NotImplementedError("Dilation > 1 not supported in BasicBlock")
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = norm_layer(planes)
        self.downsample = downsample
        self.stride = stride
        # add SE block
        self.se = SE_Block(planes, r)

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        # add SE operation
        out = self.se(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out
```

And that's it! As easy as it may sound, we have just implemented the `SEBasicBlock` in PyTorch. 

Next, for ResNet-50 and above, we perform the same steps for `Bottleneck` architecture. First, we copy it from torchvision from [here](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L75).

```python
class Bottleneck(nn.Module):
    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)
    # while original implementation places the stride at the first 1x1 convolution(self.conv1)
    # according to "Deep residual learning for image recognition"https://arxiv.org/abs/1512.03385.
    # This variant is also known as ResNet V1.5 and improves accuracy according to
    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.

    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,
                 base_width=64, dilation=1, norm_layer=None):
        super(Bottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.)) * groups
        # Both self.conv2 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv1x1(inplanes, width)
        self.bn1 = norm_layer(width)
        self.conv2 = conv3x3(width, width, stride, groups, dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = conv1x1(width, planes * self.expansion)
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out
```

Next, we add the `SE_Block` operation similar to what we did for the `BasicBlock` as below:

```python
class SEBottleneck(nn.Module):
    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)
    # while original implementation places the stride at the first 1x1 convolution(self.conv1)
    # according to "Deep residual learning for image recognition"https://arxiv.org/abs/1512.03385.
    # This variant is also known as ResNet V1.5 and improves accuracy according to
    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.

    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,
                 base_width=64, dilation=1, norm_layer=None, r=16):
        super(SEBottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.)) * groups
        # Both self.conv2 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv1x1(inplanes, width)
        self.bn1 = norm_layer(width)
        self.conv2 = conv3x3(width, width, stride, groups, dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = conv1x1(width, planes * self.expansion)
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride
        # Add SE block
        self.se = SE_Block(planes, r)

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)
        # Add SE operation
        out = self.se(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out
```

That's it! Now that we have implemented `SEBasicBlock` and `SEBottleneck` in PyTorch, we are ready to construct `SE-ResNet` architectures. As was mentioned in the paper, 

> The structure of the SE block is simple and can be used directly in existing state-of-the-art architectures by replacing components with their SE counterparts, where the performance can be effectively enhanced.

Let's do exactly this. We simply replace the components of ResNet architecture with the SE counterparts. 

But, one last step is to copy some helper functions from torchvision. These functions are present [here](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L24):

```python
def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=dilation, groups=groups, bias=False, dilation=dilation)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def _resnet(arch, block, layers, pretrained, progress, **kwargs):
    model = ResNet(block, layers, **kwargs)
    return model
```


### SEResNet-18
From torchvision [here](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L232), 

```python 
def se_resnet18(pretrained=False, progress=True, **kwargs):
    return _resnet('resnet18', SEBasicBlock, [2, 2, 2, 2], pretrained, progress,
                   **kwargs)
```

### SEResNet-34
From torchvision [here](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L244), 

```python 
def se_resnet34(pretrained=False, progress=True, **kwargs):
    return _resnet('resnet34', SEBasicBlock, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)
```

### SEResNet-50
From torchvision [here](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L256), 

```python 
def se_resnet50(pretrained=False, progress=True, **kwargs):
    return _resnet('resnet50', SEBottleneck, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)
```

### SEResNet-101
From torchvision [here](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L268), 

```python 
def se_resnet101(pretrained=False, progress=True, **kwargs):
    return _resnet('resnet101', SEBottleneck, [3, 4, 23, 3], pretrained, progress,
                   **kwargs)
```

## Conclusion 

In this blogpost, first we looked at what SE blocks are and the novel idea of channel dependencies that they introduce. Next, we looked at the **Squeeze and Excitation** operation that is used to generate per-channel weights which are then used to return the final output of the SE block. 

Finally, we looked at integration of SE block in the ResNet architecture to construct `se_resnet18`, `se_resnet34`, `se_resnet50` and `se_resnet101`. 

I hope that my explaination of SE Blocks was clear and as always - constructive feedback is always welcome at [@amaarora](https://twitter.com/amaarora). 

Also, feel free to [subscribe](https://amaarora.github.io/subscribe) to receive regular updates regarding new blog posts. Thanks for reading!
